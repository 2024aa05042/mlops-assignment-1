name: CI/CD Pipeline - Linting, Testing & Model Training

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

env:
  PYTHON_VERSION: "3.9"
  CACHE_NUMBER: 1

jobs:
  # Job 1: Code Quality & Linting
  lint:
    name: ğŸ” Code Quality & Linting
    runs-on: ubuntu-latest
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v3
      
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: deployment/requirements.txt
    
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pylint black isort
        if [ -f deployment/requirements.txt ]; then pip install -r deployment/requirements.txt; fi
        
    - name: ğŸ¯ Flake8 Linting
      run: |
        echo "Running flake8 checks..."
        flake8 src/ deployment/app/ --count --select=E9,F63,F7,F82 --show-source --statistics || true
        flake8 src/ deployment/app/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      continue-on-error: true
      
    - name: ğŸ¯ Pylint Analysis
      run: |
        echo "Running pylint checks..."
        pylint src/train_pipeline.py deployment/app/main.py --disable=all --enable=E,F --fail-under=9.0 || true
      continue-on-error: true
      
    - name: ğŸ¨ Code Format Check (Black)
      run: |
        echo "Checking code format with Black..."
        black --check src/ deployment/app/ || true
      continue-on-error: true
      
    - name: ğŸ“¤ Upload Lint Report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: lint-reports
        path: |
          flake8_report.txt
          pylint_report.txt
        retention-days: 30

  # Job 2: Unit Tests & Code Coverage
  test:
    name: ğŸ§ª Unit Tests & Coverage
    runs-on: ubuntu-latest
    needs: lint
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v3
      
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: deployment/requirements.txt
    
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-xdist pandas numpy scikit-learn joblib
        if [ -f deployment/requirements.txt ]; then pip install -r deployment/requirements.txt; fi
        
    - name: ğŸ§ª Run Unit Tests
      run: |
        echo "Running pytest suite..."
        pytest tests/ -v --tb=short --cov=src --cov=deployment/app --cov-report=xml --cov-report=html:coverage_report -m "not slow"
      continue-on-error: false
      
    - name: ğŸ“Š Generate Coverage Report
      run: |
        echo "Generating coverage summary..."
        coverage report -m > coverage_summary.txt || true
        cat coverage_summary.txt
      continue-on-error: true
      
    - name: ğŸ“¤ Upload Test Results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results
        path: |
          coverage_report/
          coverage_summary.txt
          .coverage
        retention-days: 30
        
    - name: ğŸ“¤ Upload Coverage Report
      if: always()
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  # Job 3: Model Training & Evaluation
  train:
    name: ğŸ¤– Model Training & Evaluation
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v3
      
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: deployment/requirements.txt
    
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas numpy scikit-learn joblib mlflow
        if [ -f deployment/requirements.txt ]; then pip install -r deployment/requirements.txt; fi
        
    - name: ğŸ“¥ Download Test Artifacts
      uses: actions/download-artifact@v3
      with:
        name: test-results
        path: test-artifacts/
      continue-on-error: true
    
    - name: ğŸ¤– Train Model
      run: |
        echo "Starting model training pipeline..."
        python src/train_pipeline.py 2>&1 | tee training_log.txt
      continue-on-error: false
      
    - name: ğŸ“Š Generate Training Report
      run: |
        cat << 'EOF' > generate_training_report.py
        import os
        import json
        from datetime import datetime
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "status": "completed",
            "model_path": "models/heart_disease_pipeline_prod.joblib",
            "metrics": {
                "model_type": "RandomForestClassifier",
                "n_estimators": 200,
                "max_depth": 10
            }
        }
        
        # Check if model was created
        if os.path.exists("models/heart_disease_pipeline_prod.joblib"):
            model_size = os.path.getsize("models/heart_disease_pipeline_prod.joblib") / (1024*1024)
            report["metrics"]["model_size_mb"] = round(model_size, 2)
            report["status"] = "success"
        else:
            report["status"] = "failed"
        
        with open("training_report.json", "w") as f:
            json.dump(report, f, indent=2)
        
        print("Training report generated:")
        print(json.dumps(report, indent=2))
        EOF
        python generate_training_report.py
      continue-on-error: true
      
    - name: ğŸ“¤ Upload Model Artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: model-artifacts
        path: |
          models/
          training_log.txt
          training_report.json
        retention-days: 90
        
    - name: ğŸ“Š Model Performance Summary
      if: always()
      run: |
        echo "=== Model Training Summary ===" 
        [ -f training_report.json ] && cat training_report.json || echo "No training report found"
        [ -f training_log.txt ] && tail -20 training_log.txt || echo "No training log found"

  # Job 4: Integration Tests (Optional but recommended)
  integration-test:
    name: ğŸ”— Integration Tests
    runs-on: ubuntu-latest
    needs: train
    if: success()
    
    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v3
      
    - name: ğŸ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
        cache-dependency-path: deployment/requirements.txt
    
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pandas numpy scikit-learn joblib fastapi
        if [ -f deployment/requirements.txt ]; then pip install -r deployment/requirements.txt; fi
        
    - name: ğŸ“¥ Download Model Artifacts
      uses: actions/download-artifact@v3
      with:
        name: model-artifacts
        path: ./
      continue-on-error: true
    
    - name: ğŸ”— Run Integration Tests
      run: |
        echo "Running integration tests..."
        python -c "
        import joblib
        import pandas as pd
        import os
        
        # Check if model exists
        if os.path.exists('models/heart_disease_pipeline_prod.joblib'):
            print('âœ… Model file found')
            model = joblib.load('models/heart_disease_pipeline_prod.joblib')
            print('âœ… Model loaded successfully')
            
            # Test prediction
            test_data = pd.DataFrame({
                'age': [63],
                'sex': [1],
                'cp': [3],
                'trestbps': [145],
                'chol': [233],
                'fbs': [1],
                'restecg': [0],
                'thalach': [150],
                'exang': [0],
                'oldpeak': [2.3],
                'slope': [0],
                'ca': [0],
                'thal': [1]
            })
            
            prediction = model.predict(test_data)
            proba = model.predict_proba(test_data)
            print(f'âœ… Prediction: {prediction[0]}')
            print(f'âœ… Probability: {proba[0][1]:.4f}')
        else:
            print('âŒ Model file not found')
            exit(1)
        " 2>&1 | tee integration_test.txt
      continue-on-error: false

  # Job 5: Summary Report
  summary:
    name: ğŸ“‹ Pipeline Summary
    runs-on: ubuntu-latest
    if: always()
    needs: [lint, test, train, integration-test]
    
    steps:
    - name: ğŸ“¥ Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: all-artifacts/
      continue-on-error: true
    
    - name: ğŸ“Š Generate Pipeline Summary
      run: |
        cat << 'EOF' > pipeline_summary.md
        # ğŸš€ MLOps Pipeline Execution Summary
        
        ## Pipeline Status
        - **Linting**: ${{ needs.lint.result }}
        - **Unit Tests**: ${{ needs.test.result }}
        - **Model Training**: ${{ needs.train.result }}
        - **Integration Tests**: ${{ needs.integration-test.result }}
        
        ## Artifacts Generated
        - Lint Reports (flake8, pylint, black)
        - Unit Test Coverage Reports
        - Model Artifacts & Training Logs
        - Integration Test Results
        
        ## Execution Details
        - **Trigger**: ${{ github.event_name }}
        - **Branch**: ${{ github.ref }}
        - **Commit**: ${{ github.sha }}
        - **Timestamp**: $(date -u +'%Y-%m-%dT%H:%M:%SZ')
        
        ## Next Steps
        1. Review lint and test reports in artifacts
        2. Check model performance metrics
        3. Deploy to staging if all checks pass
        
        ---
        Generated by GitHub Actions
        EOF
        cat pipeline_summary.md
      
    - name: ğŸ“¤ Upload Pipeline Summary
      uses: actions/upload-artifact@v3
      with:
        name: pipeline-summary
        path: |
          pipeline_summary.md
          all-artifacts/
        retention-days: 90

  # Notification (Optional)
  notify:
    name: ğŸ”” Notify Status
    runs-on: ubuntu-latest
    if: always()
    needs: [lint, test, train]
    
    steps:
    - name: ğŸ“§ Pipeline Status Notification
      run: |
        echo "Pipeline Execution Complete!"
        echo "Lint Status: ${{ needs.lint.result }}"
        echo "Test Status: ${{ needs.test.result }}"
        echo "Train Status: ${{ needs.train.result }}"
        
        if [ "${{ needs.lint.result }}" == "success" ] && [ "${{ needs.test.result }}" == "success" ] && [ "${{ needs.train.result }}" == "success" ]; then
          echo "âœ… All checks passed!"
        else
          echo "âš ï¸ Some checks failed - review logs"
        fi
